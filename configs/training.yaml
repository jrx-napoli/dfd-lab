data:
  # Paths to shard datasets
  shards_dir: "data/processed/FakeAVCeleb_v1.2/shards"
  train_index: "index_train.csv"
  val_index: "index_val.csv"
  
  # Data loading configuration
  batch_size: 16  # Mini-batch size for training
  num_workers: 0  # Number of data loading workers (0 for IterableDataset)
  pin_memory: true
  
  # Optional: limit samples for debugging
  max_train_samples: null  # Set to integer to limit training samples
  max_val_samples: null    # Set to integer to limit validation samples

model:
  # Model type - can be extended to support any BaseDetector subclass
  name: "XceptionMaxFusionDetector"  # Options: "XceptionMaxFusionDetector", or any other detector in src/models
  module_path: "src.models.xception"  # Python import path to model module
  num_classes: 2
  
  # Model-specific parameters (passed to __init__ as kwargs)
  model_params: {}

training:
  epochs: 50
  grad_clip: 1.0  # Gradient clipping (0 to disable)
  
  early_stopping:
    patience: 10
    min_delta: 0.001
  
  optimizer:
    name: "adam"  # Options: "adam", "sgd", "adamw"
    learning_rate: 0.0001
    weight_decay: 0.0001
    momentum: 0.9  # For SGD
  
  scheduler:
    name: "cosine"  # Options: "cosine", "step", "plateau", "none"
    warmup_epochs: 0
    min_lr: 0.000001

loss:
  name: "cross_entropy"  # Options: "cross_entropy", "focal"
  focal_gamma: 2.0  # For focal loss (if used)

logging:
  log_dir: "logs"
  save_frequency: 5  # Save checkpoint every N epochs
  metrics:
    - "accuracy"
    - "precision"
    - "recall"
    - "f1_score"
    - "auc_roc"

checkpointing:
  dir: "checkpoints"
  save_best_only: false  # If false, saves checkpoints every save_frequency epochs
  monitor: "f1_score"  # Metric to monitor for best model (without "val_" prefix)
  mode: "max"  # "max" or "min"

# Device configuration
device: "cuda"  # Options: "cuda", "cpu", "cuda:0", "cuda:1", etc.

# Seed for reproducibility
seed: 42 